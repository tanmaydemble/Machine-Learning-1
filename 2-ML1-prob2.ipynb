{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bddeda4",
   "metadata": {},
   "source": [
    "# Problem 2 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecade587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2494b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
      "0           1  221900.0         3       1.00         1180      5650     1.0   \n",
      "1           2  538000.0         3       2.25         2570      7242     2.0   \n",
      "2           3  180000.0         2       1.00          770     10000     1.0   \n",
      "3           4  604000.0         4       3.00         1960      5000     1.0   \n",
      "4           5  510000.0         3       2.00         1680      8080     1.0   \n",
      "\n",
      "   waterfront  view  condition  grade  sqft_above  sqft_basement  yr_built  \\\n",
      "0           0     0          3      7        1180              0      1955   \n",
      "1           0     0          3      7        2170            400      1951   \n",
      "2           0     0          3      6         770              0      1933   \n",
      "3           0     0          5      7        1050            910      1965   \n",
      "4           0     0          3      8        1680              0      1987   \n",
      "\n",
      "   yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
      "0             0    98178  47.5112 -122.257           1340        5650  \n",
      "1          1991    98125  47.7210 -122.319           1690        7639  \n",
      "2             0    98028  47.7379 -122.233           2720        8062  \n",
      "3             0    98136  47.5208 -122.393           1360        5000  \n",
      "4             0    98074  47.6168 -122.045           1800        7503  \n",
      "------------------\n",
      "   Unnamed: 0          id             date     price  bedrooms  bathrooms  \\\n",
      "0        1001  3131200640  20150427T000000  700000.0         4       2.00   \n",
      "1        1002   984000710  20141022T000000  270000.0         3       2.00   \n",
      "2        1003  4167300350  20140508T000000  258000.0         4       1.75   \n",
      "3        1004  2826049282  20140614T000000  530000.0         3       2.50   \n",
      "4        1005  8946750030  20141218T000000  245000.0         3       2.25   \n",
      "\n",
      "   sqft_living  sqft_lot  floors  waterfront  ...  grade  sqft_above  \\\n",
      "0         1830      4590     2.0           0  ...      8        1830   \n",
      "1         1560      8853     1.0           0  ...      7        1560   \n",
      "2         1730      8320     1.0           0  ...      7        1230   \n",
      "3         1930      7214     2.0           0  ...      8        1930   \n",
      "4         1422      3677     2.0           0  ...      7        1422   \n",
      "\n",
      "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
      "0              0      1908             0    98105  47.6593 -122.327   \n",
      "1              0      1967             0    98058  47.4312 -122.171   \n",
      "2            500      1977             0    98023  47.3270 -122.361   \n",
      "3              0      2005             0    98125  47.7191 -122.309   \n",
      "4              0      2012             0    98092  47.3204 -122.178   \n",
      "\n",
      "   sqft_living15  sqft_lot15  \n",
      "0           1650        4590  \n",
      "1           1610        8750  \n",
      "2           1840        9800  \n",
      "3           1930        7266  \n",
      "4           1677        3677  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# importing the training dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\tanma\\\\Downloads\\\\train.csv')\n",
    "print(df.head())\n",
    "print(\"------------------\")\n",
    "\n",
    "# importing the test dataset\n",
    "test_df = pd.read_csv('C:\\\\Users\\\\tanma\\\\Downloads\\\\test.csv')\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1718e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price  bedrooms  bathrooms  sqft_living  sqft_lot    floors  waterfront  \\\n",
      "0  221.9 -0.409823  -1.449888    -0.981646 -0.312717 -0.863477   -0.089803   \n",
      "1  538.0 -0.409823   0.283184     0.584578 -0.257719  1.070402   -0.089803   \n",
      "2  180.0 -1.584103  -1.449888    -1.443625 -0.162440 -0.863477   -0.089803   \n",
      "3  604.0  0.764456   1.323028    -0.102758 -0.335172 -0.863477   -0.089803   \n",
      "4  510.0 -0.409823  -0.063430    -0.418256 -0.228769 -0.863477   -0.089803   \n",
      "\n",
      "       view  condition     grade  sqft_above  sqft_basement  yr_built  \\\n",
      "0 -0.309908  -0.673452 -0.522576   -0.722231      -0.667586 -0.498602   \n",
      "1 -0.309908  -0.673452 -0.522576    0.531438       0.219976 -0.640563   \n",
      "2 -0.309908  -0.673452 -1.384913   -1.241427      -0.667586 -1.279387   \n",
      "3 -0.309908   2.229358 -0.522576   -0.886854       1.351617 -0.143700   \n",
      "4 -0.309908  -0.673452  0.339761   -0.089065      -0.667586  0.637085   \n",
      "\n",
      "   yr_renovated       lat      long  sqft_living15  sqft_lot15  \n",
      "0     -0.206760 -0.270431 -0.355193      -0.965637   -0.312858  \n",
      "1      4.828896  1.211219 -0.799830      -0.443330   -0.233556  \n",
      "2     -0.206760  1.330570 -0.183076       1.093745   -0.216690  \n",
      "3     -0.206760 -0.202634 -1.330526      -0.935790   -0.338774  \n",
      "4     -0.206760  0.475338  1.165179      -0.279176   -0.238978  \n",
      "---------------------\n",
      "   price  bedrooms  bathrooms  sqft_living  sqft_lot    floors  waterfront  \\\n",
      "0  700.0  0.619711  -0.127658    -0.285135 -0.195711  1.129732   -0.114766   \n",
      "1  270.0 -0.452453  -0.127658    -0.572912 -0.123022 -0.848783   -0.114766   \n",
      "2  258.0  0.619711  -0.442864    -0.391719 -0.132110 -0.848783   -0.114766   \n",
      "3  530.0 -0.452453   0.502753    -0.178551 -0.150969  1.129732   -0.114766   \n",
      "4  245.0 -0.452453   0.187547    -0.719998 -0.211279  1.129732   -0.114766   \n",
      "\n",
      "       view  condition     grade  sqft_above  sqft_basement  yr_built  \\\n",
      "0 -0.329112  -0.691473  0.314595    0.047079      -0.681769 -2.129209   \n",
      "1 -0.329112  -0.691473 -0.515471   -0.267612      -0.681769 -0.019169   \n",
      "2 -0.329112  -0.691473 -0.515471   -0.652235       0.425308  0.338465   \n",
      "3 -0.329112  -0.691473  0.314595    0.163631      -0.681769  1.339840   \n",
      "4 -0.329112  -0.691473 -0.515471   -0.428454      -0.681769  1.590183   \n",
      "\n",
      "   yr_renovated       lat      long  sqft_living15  sqft_lot15  \n",
      "0      -0.22941  0.704235 -0.831615      -0.492826   -0.324978  \n",
      "1      -0.22941 -0.963278  0.323715      -0.550422   -0.177050  \n",
      "2      -0.22941 -1.725026 -1.083418      -0.219247   -0.139713  \n",
      "3      -0.22941  1.141400 -0.698308      -0.089657   -0.229821  \n",
      "4      -0.22941 -1.773275  0.271873      -0.453949   -0.357444  \n"
     ]
    }
   ],
   "source": [
    "#removing the unnamed column\n",
    "df.drop(columns=df.columns[df.columns.str.contains('^Unnamed')], inplace=True)\n",
    "test_df.drop(columns=test_df.columns[test_df.columns.str.contains('^Unnamed')], inplace=True)\n",
    "df.drop(['zipcode'], axis=1, inplace=True)\n",
    "test_df.drop(['id', 'date'], axis=1, inplace=True)\n",
    "test_df.drop(['zipcode'], axis=1, inplace=True)\n",
    "\n",
    "#scaling the features except price because it is the response variable\n",
    "features_to_scale = df.columns.difference(['price'])  \n",
    "scaler = StandardScaler()\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "test_df[features_to_scale] = scaler.fit_transform(test_df[features_to_scale])\n",
    "\n",
    "# dividing the price by 1000 because it redues the MSE\n",
    "df['price'] = df['price'] / 1000\n",
    "print(df.head())\n",
    "print(\"---------------------\")\n",
    "test_df['price'] = test_df['price'] / 1000\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b98476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      221.90\n",
      "1      538.00\n",
      "2      180.00\n",
      "3      604.00\n",
      "4      510.00\n",
      "        ...  \n",
      "995    291.00\n",
      "996    199.95\n",
      "997    553.50\n",
      "998    189.95\n",
      "999    289.00\n",
      "Name: price, Length: 1000, dtype: float64\n",
      "   bedrooms  bathrooms  sqft_living  sqft_lot    floors  waterfront      view  \\\n",
      "0 -0.409823  -1.449888    -0.981646 -0.312717 -0.863477   -0.089803 -0.309908   \n",
      "1 -0.409823   0.283184     0.584578 -0.257719  1.070402   -0.089803 -0.309908   \n",
      "2 -1.584103  -1.449888    -1.443625 -0.162440 -0.863477   -0.089803 -0.309908   \n",
      "3  0.764456   1.323028    -0.102758 -0.335172 -0.863477   -0.089803 -0.309908   \n",
      "4 -0.409823  -0.063430    -0.418256 -0.228769 -0.863477   -0.089803 -0.309908   \n",
      "\n",
      "   condition     grade  sqft_above  sqft_basement  yr_built  yr_renovated  \\\n",
      "0  -0.673452 -0.522576   -0.722231      -0.667586 -0.498602     -0.206760   \n",
      "1  -0.673452 -0.522576    0.531438       0.219976 -0.640563      4.828896   \n",
      "2  -0.673452 -1.384913   -1.241427      -0.667586 -1.279387     -0.206760   \n",
      "3   2.229358 -0.522576   -0.886854       1.351617 -0.143700     -0.206760   \n",
      "4  -0.673452  0.339761   -0.089065      -0.667586  0.637085     -0.206760   \n",
      "\n",
      "        lat      long  sqft_living15  sqft_lot15  \n",
      "0 -0.270431 -0.355193      -0.965637   -0.312858  \n",
      "1  1.211219 -0.799830      -0.443330   -0.233556  \n",
      "2  1.330570 -0.183076       1.093745   -0.216690  \n",
      "3 -0.202634 -1.330526      -0.935790   -0.338774  \n",
      "4  0.475338  1.165179      -0.279176   -0.238978  \n",
      "0      700.0\n",
      "1      270.0\n",
      "2      258.0\n",
      "3      530.0\n",
      "4      245.0\n",
      "       ...  \n",
      "995    307.0\n",
      "996    194.0\n",
      "997    180.0\n",
      "998    465.0\n",
      "999    722.5\n",
      "Name: price, Length: 1000, dtype: float64\n",
      "   bedrooms  bathrooms  sqft_living  sqft_lot    floors  waterfront      view  \\\n",
      "0  0.619711  -0.127658    -0.285135 -0.195711  1.129732   -0.114766 -0.329112   \n",
      "1 -0.452453  -0.127658    -0.572912 -0.123022 -0.848783   -0.114766 -0.329112   \n",
      "2  0.619711  -0.442864    -0.391719 -0.132110 -0.848783   -0.114766 -0.329112   \n",
      "3 -0.452453   0.502753    -0.178551 -0.150969  1.129732   -0.114766 -0.329112   \n",
      "4 -0.452453   0.187547    -0.719998 -0.211279  1.129732   -0.114766 -0.329112   \n",
      "\n",
      "   condition     grade  sqft_above  sqft_basement  yr_built  yr_renovated  \\\n",
      "0  -0.691473  0.314595    0.047079      -0.681769 -2.129209      -0.22941   \n",
      "1  -0.691473 -0.515471   -0.267612      -0.681769 -0.019169      -0.22941   \n",
      "2  -0.691473 -0.515471   -0.652235       0.425308  0.338465      -0.22941   \n",
      "3  -0.691473  0.314595    0.163631      -0.681769  1.339840      -0.22941   \n",
      "4  -0.691473 -0.515471   -0.428454      -0.681769  1.590183      -0.22941   \n",
      "\n",
      "        lat      long  sqft_living15  sqft_lot15  \n",
      "0  0.704235 -0.831615      -0.492826   -0.324978  \n",
      "1 -0.963278  0.323715      -0.550422   -0.177050  \n",
      "2 -1.725026 -1.083418      -0.219247   -0.139713  \n",
      "3  1.141400 -0.698308      -0.089657   -0.229821  \n",
      "4 -1.773275  0.271873      -0.453949   -0.357444  \n"
     ]
    }
   ],
   "source": [
    "# separating the response variable from the dataset\n",
    "price_res = df['price']\n",
    "print(price_res)\n",
    "df.drop('price', axis=1, inplace=True)\n",
    "print(df.head())\n",
    "\n",
    "price_res_test = test_df['price']\n",
    "print(price_res_test)\n",
    "test_df.drop('price', axis=1, inplace=True)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc78f8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "model = LinearRegression()\n",
    "model.fit(df, price_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3abfae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7265334318706018\n",
      "RMSE: 177.44342133704166\n"
     ]
    }
   ],
   "source": [
    "# getting the predicted values\n",
    "price_pred = model.predict(df)\n",
    "\n",
    "print(\"R^2:\", r2_score(price_res, price_pred))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, price_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1049b",
   "metadata": {},
   "source": [
    "# Problem 2 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ab32f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.6414235000248596\n",
      "RMSE: 244.50841614864035\n"
     ]
    }
   ],
   "source": [
    "# predicting on the test data\n",
    "price_pred_test = model.predict(test_df)\n",
    "\n",
    "# checking R^2 and MSE on the testing data\n",
    "print(\"R^2:\", r2_score(price_res_test, price_pred_test))\n",
    "print(\"RMSE:\", mean_squared_error(price_res_test, price_pred_test, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99bda2",
   "metadata": {},
   "source": [
    "# Problem 2 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40971b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedrooms        -12.521962\n",
      "bathrooms        18.527633\n",
      "sqft_living      56.748837\n",
      "sqft_lot         10.881868\n",
      "floors            8.043721\n",
      "waterfront       63.742900\n",
      "view             48.200109\n",
      "condition        12.964269\n",
      "grade            92.231475\n",
      "sqft_above       48.290089\n",
      "sqft_basement    27.137032\n",
      "yr_built        -67.643117\n",
      "yr_renovated     17.271380\n",
      "lat              78.375737\n",
      "long             -1.035203\n",
      "sqft_living15    45.577658\n",
      "sqft_lot15      -12.930091\n",
      "dtype: float64\n",
      "\n",
      "Features with large positive coefficients, such as grade (92.23), waterfront (63.74), and lat (latitude, 78.38), contribute significantly to increasing the property's price.\n",
      "Some features negatively impact the price, notably yr_built (-67.64) and bedrooms (-12.52). The negative coefficient for yr_built suggests that older homes might be valued less, possibly due to outdated features or wear. \n",
      "The R^2 value of approximately 0.73 on the training data indicates that the model explains around 73% of the variance in housing prices, which is a decent fit. The RMSE of 177.44 means the model's predictions are, on average, around $177,440 off from the actual sale prices in the training set.\n",
      "The R^2 value drops to about 0.64 on the test set, suggesting the model explains 64% of the variance in the data. This decrease in R^2 and increase in RMSE to 244.51 on the test set compared to the training set indicate the model doesn't generalize as well to unseen data, which is a sign of overfitting.\n",
      "Overall, the fit of the model is mediocre as we are only able to explain 64% of the variance in the data and our model is also overfitting the data.\n"
     ]
    }
   ],
   "source": [
    "# interpreting the results\n",
    "coefficients = model.coef_\n",
    "feature_names = df.columns\n",
    "coefficients_series = pd.Series(coefficients, index=feature_names)\n",
    "print(coefficients_series)\n",
    "print(\"\\nFeatures with large positive coefficients, such as grade (92.23), waterfront (63.74), and lat (latitude, 78.38), contribute significantly to increasing the property's price.\")\n",
    "print(\"Some features negatively impact the price, notably yr_built (-67.64) and bedrooms (-12.52). The negative coefficient for yr_built suggests that older homes might be valued less, possibly due to outdated features or wear. \")\n",
    "print(\"The R^2 value of approximately 0.73 on the training data indicates that the model explains around 73% of the variance in housing prices, which is a decent fit. The RMSE of 177.44 means the model's predictions are, on average, around $177,440 off from the actual sale prices in the training set.\")\n",
    "print(\"The R^2 value drops to about 0.64 on the test set, suggesting the model explains 64% of the variance in the data. This decrease in R^2 and increase in RMSE to 244.51 on the test set compared to the training set indicate the model doesn't generalize as well to unseen data, which is a sign of overfitting.\")\n",
    "print(\"Overall, the fit of the model is mediocre as we are only able to explain 64% of the variance in the data and our model is also overfitting the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53172db",
   "metadata": {},
   "source": [
    "# Problem 3 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39b3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_regression(X, y):\n",
    "    theta = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149695a",
   "metadata": {},
   "source": [
    "# Problem 3 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "414e50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[520.414834   -12.52196187  18.52763251  56.7488368   10.88186845\n",
      "   8.04372084  63.74289956  48.20010852  12.96426936  92.23147482\n",
      "  48.29008886  27.13703247 -67.64311741  17.27137953  78.37573693\n",
      "  -1.03520308  45.57765781 -12.93009098]\n"
     ]
    }
   ],
   "source": [
    "intercept = np.ones((df.shape[0], 1))\n",
    "df_with_intercept = np.hstack((intercept, df))\n",
    "\n",
    "theta = my_regression(df_with_intercept, price_res)\n",
    "\n",
    "print(theta)\n",
    "#feature_names_with_intercept = ['Intercept'] + feature_names\n",
    "#theta_series = pd.Series(theta, index=feature_names_with_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26720237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 271.40656164  729.11848416  427.65718493  426.19973209  450.47662131\n",
      " 1380.79098139]\n",
      "MSE: 177.44342133704163\n",
      "R^2: 0.7265334318706018\n"
     ]
    }
   ],
   "source": [
    "# checking the metrics on the training data\n",
    "predicted_y = df_with_intercept.dot(theta)\n",
    "print(predicted_y[:6])\n",
    "\n",
    "mse = mean_squared_error(price_res, predicted_y, squared=False)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R^2:\", r2_score(price_res, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100b59f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[659.57077903 270.63107137 205.19237484 513.45305665 109.74866765\n",
      " 526.3913025 ]\n",
      "MSE: 244.5084161486395\n",
      "R^2: 0.6414235000248621\n",
      "The results given by my multiple regression function are the same to the regression function implemented by scikit learn\n"
     ]
    }
   ],
   "source": [
    "# checking the metrics on the test data\n",
    "df_with_intercept_test = np.hstack((intercept, test_df))\n",
    "my_predicted_y_test = df_with_intercept_test.dot(theta)\n",
    "print(my_predicted_y_test[:6])\n",
    "\n",
    "mse_test = mean_squared_error(price_res_test, my_predicted_y_test, squared=False)\n",
    "print(\"MSE:\", mse_test)\n",
    "print(\"R^2:\", r2_score(price_res_test, my_predicted_y_test))\n",
    "print(\"The results given by my multiple regression function are the same to the regression function implemented by scikit learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e32e2",
   "metadata": {},
   "source": [
    "# Problem 4 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c512906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_poly_regression(X, y, p):\n",
    "    X_poly = np.vstack([X**i for i in range(p + 1)]).T\n",
    "    theta = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980740b",
   "metadata": {},
   "source": [
    "# Problem 4 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114a377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the sqft_living column\n",
    "sqft_living = df['sqft_living']\n",
    "sqft_living_test = test_df['sqft_living']\n",
    "\n",
    "theta_poly_1 = my_poly_regression(sqft_living, price_res, 1)\n",
    "theta_poly_2 = my_poly_regression(sqft_living, price_res, 2)\n",
    "theta_poly_3 = my_poly_regression(sqft_living, price_res, 3)\n",
    "theta_poly_4 = my_poly_regression(sqft_living, price_res, 4)\n",
    "theta_poly_5 = my_poly_regression(sqft_living, price_res, 5)\n",
    "\n",
    "X_poly_1 = np.vstack([sqft_living**i for i in range(2)]).T\n",
    "X_poly_2 = np.vstack([sqft_living**i for i in range(3)]).T\n",
    "X_poly_3 = np.vstack([sqft_living**i for i in range(4)]).T\n",
    "X_poly_4 = np.vstack([sqft_living**i for i in range(5)]).T\n",
    "X_poly_5 = np.vstack([sqft_living**i for i in range(6)]).T\n",
    "\n",
    "X_poly_1_test = np.vstack([sqft_living_test**i for i in range(2)]).T\n",
    "X_poly_2_test = np.vstack([sqft_living_test**i for i in range(3)]).T\n",
    "X_poly_3_test = np.vstack([sqft_living_test**i for i in range(4)]).T\n",
    "X_poly_4_test = np.vstack([sqft_living_test**i for i in range(5)]).T\n",
    "X_poly_5_test = np.vstack([sqft_living_test**i for i in range(6)]).T\n",
    "\n",
    "predicted_y_poly_1 = X_poly_1.dot(theta_poly_1)\n",
    "predicted_y_poly_2 = X_poly_2.dot(theta_poly_2)\n",
    "predicted_y_poly_3 = X_poly_3.dot(theta_poly_3)\n",
    "predicted_y_poly_4 = X_poly_4.dot(theta_poly_4)\n",
    "predicted_y_poly_5 = X_poly_5.dot(theta_poly_5)\n",
    "\n",
    "predicted_y_poly_1_test = X_poly_1_test.dot(theta_poly_1)\n",
    "predicted_y_poly_2_test = X_poly_2_test.dot(theta_poly_2)\n",
    "predicted_y_poly_3_test = X_poly_3_test.dot(theta_poly_3)\n",
    "predicted_y_poly_4_test = X_poly_4_test.dot(theta_poly_4)\n",
    "predicted_y_poly_5_test = X_poly_5_test.dot(theta_poly_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a7a4bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Index     MSE_col   r^2_col  MSE_col_test  r^2_col_test\n",
      "0  poly_1  240.722924  0.496709    424.189757      0.460724\n",
      "1  poly_2  234.142403  0.523849    438.022885      0.555984\n",
      "2  poly_3  231.916353  0.532860    421.818891      0.421775\n",
      "3  poly_4  229.773312  0.541453    427.834962     -0.115057\n",
      "4  poly_5  229.403819  0.542927    497.178647     -1.015995\n",
      "\n",
      " Comments:\n",
      "The Mean Squared Error on the training set decreases as the degree of the polynomial increases from 1 to 5. This trend suggests that higher-degree polynomial models fit the training data more closely, capturing more of its variance.\n",
      "\n",
      "\n",
      "R square value increases with the polynomial degree, indicating an improvement in the proportion of the variance in the dependent variable that is predictable from the independent variable\n",
      "\n",
      "\n",
      "The Mean Squared Error on the test set initially decreases from poly_1 to poly_3 but then increases for poly_4 and poly_5. This suggests that up to a certain point (degree 3 in this case), the model's ability to generalize improves. However, beyond this point, the complexity added by higher-degree terms causes the model to overfit the training data, worsening its performance on the test set.\n",
      "\n",
      "\n",
      "The R square value improves from poly_1 to poly_2, indicating better model performance. However, it starts to decline at poly_3 and becomes negative for poly_4 and poly_5.\n",
      "\n",
      "\n",
      "This is a clear sign of overfitting, where the model captures the noise in the training data instead of the underlying relationship, leading to poor predictive performance on new data.\n"
     ]
    }
   ],
   "source": [
    "MSE_poly_1 = mean_squared_error(price_res, predicted_y_poly_1, squared=False)\n",
    "r2_poly_1 = r2_score(price_res, predicted_y_poly_1)\n",
    "\n",
    "MSE_poly_2 = mean_squared_error(price_res, predicted_y_poly_2, squared=False)\n",
    "r2_poly_2 = r2_score(price_res, predicted_y_poly_2)\n",
    "\n",
    "MSE_poly_3 = mean_squared_error(price_res, predicted_y_poly_3, squared=False)\n",
    "r2_poly_3 = r2_score(price_res, predicted_y_poly_3)\n",
    "\n",
    "MSE_poly_4 = mean_squared_error(price_res, predicted_y_poly_4, squared=False)\n",
    "r2_poly_4 = r2_score(price_res, predicted_y_poly_4)\n",
    "\n",
    "MSE_poly_5 = mean_squared_error(price_res, predicted_y_poly_5, squared=False)\n",
    "r2_poly_5 = r2_score(price_res, predicted_y_poly_5)\n",
    "\n",
    "MSE_poly_1_test = mean_squared_error(price_res, predicted_y_poly_1_test, squared=False)\n",
    "r2_poly_1_test = r2_score(price_res_test, predicted_y_poly_1_test)\n",
    "\n",
    "MSE_poly_2_test = mean_squared_error(price_res, predicted_y_poly_2_test, squared=False)\n",
    "r2_poly_2_test = r2_score(price_res_test, predicted_y_poly_2_test)\n",
    "\n",
    "MSE_poly_3_test = mean_squared_error(price_res, predicted_y_poly_3_test, squared=False)\n",
    "r2_poly_3_test = r2_score(price_res_test, predicted_y_poly_3_test)\n",
    "\n",
    "MSE_poly_4_test = mean_squared_error(price_res, predicted_y_poly_4_test, squared=False)\n",
    "r2_poly_4_test = r2_score(price_res_test, predicted_y_poly_4_test)\n",
    "\n",
    "MSE_poly_5_test = mean_squared_error(price_res, predicted_y_poly_5_test, squared=False)\n",
    "r2_poly_5_test = r2_score(price_res_test, predicted_y_poly_5_test)\n",
    "\n",
    "data = {\n",
    "    'Index': ['poly_1', 'poly_2', 'poly_3', 'poly_4', 'poly_5'],\n",
    "    'MSE_col': [MSE_poly_1, MSE_poly_2, MSE_poly_3, MSE_poly_4, MSE_poly_5],\n",
    "    'r^2_col': [r2_poly_1, r2_poly_2, r2_poly_3, r2_poly_4, r2_poly_5],\n",
    "    'MSE_col_test': [MSE_poly_1_test, MSE_poly_2_test, MSE_poly_3_test, MSE_poly_4_test, MSE_poly_5_test],\n",
    "    'r^2_col_test': [r2_poly_1_test, r2_poly_2_test, r2_poly_3_test, r2_poly_4_test, r2_poly_5_test]\n",
    "}\n",
    "\n",
    "metrics_table = pd.DataFrame(data)\n",
    "print(metrics_table)\n",
    "\n",
    "print(\"\\n Comments:\")\n",
    "print(\"The Mean Squared Error on the training set decreases as the degree of the polynomial increases from 1 to 5. This trend suggests that higher-degree polynomial models fit the training data more closely, capturing more of its variance.\")\n",
    "print(\"\\n\")\n",
    "print(\"R square value increases with the polynomial degree, indicating an improvement in the proportion of the variance in the dependent variable that is predictable from the independent variable\")\n",
    "print(\"\\n\")\n",
    "print(\"The Mean Squared Error on the test set initially decreases from poly_1 to poly_3 but then increases for poly_4 and poly_5. This suggests that up to a certain point (degree 3 in this case), the model's ability to generalize improves. However, beyond this point, the complexity added by higher-degree terms causes the model to overfit the training data, worsening its performance on the test set.\")\n",
    "print(\"\\n\")\n",
    "print(\"The R square value improves from poly_1 to poly_2, indicating better model performance. However, it starts to decline at poly_3 and becomes negative for poly_4 and poly_5.\")\n",
    "print(\"\\n\")\n",
    "print(\"This is a clear sign of overfitting, where the model captures the noise in the training data instead of the underlying relationship, leading to poor predictive performance on new data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6dd559",
   "metadata": {},
   "source": [
    "# Problem 5 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5faa480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing gradient descent for linear regression\n",
    "def my_gd(X, y, alpha, iterations):\n",
    "    n_samples, n_features = X.shape\n",
    "    X_b = np.c_[np.ones((n_samples, 1)), X]\n",
    "    w = np.zeros(n_features + 1)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        predictions = X_b.dot(w)\n",
    "        errors = predictions - y\n",
    "        gradients = 2/n_samples * X_b.T.dot(errors)\n",
    "        w -= alpha * gradients\n",
    "        \n",
    "        #cost = math.sqrt((1/n_samples) * np.sum(errors ** 2))\n",
    "        #print(f\"Iteration {i + 1}, Cost {cost}\")\n",
    "        #print(f\"Weights {w}\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5275b5",
   "metadata": {},
   "source": [
    "# Problem 5 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "465d395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 iterations, alpha = 0.01\n",
      "R^2: -1.0473645656795694\n",
      "RMSE: 485.5180426875398\n",
      "\n",
      "\n",
      "10 iterations, alpha = 0.1\n",
      "R^2: 0.6951019311849285\n",
      "RMSE: 187.36355550689277\n",
      "\n",
      "\n",
      "10 iterations, alpha = 0.5\n",
      "R^2: -1264634536089.7993\n",
      "RMSE: 381584127.4753253\n",
      "\n",
      "\n",
      "50 iterations, alpha = 0.01\n",
      "R^2: 0.394457093114297\n",
      "RMSE: 264.0463953494728\n",
      "\n",
      "\n",
      "50 iterations, alpha = 0.1\n",
      "R^2: 0.7264370818894943\n",
      "RMSE: 177.47467774393246\n",
      "\n",
      "\n",
      "50 iterations, alpha = 0.5\n",
      "R^2: -2.885941549152925e+124\n",
      "RMSE: 5.764365998470874e+64\n",
      "\n",
      "\n",
      "100 iterations, alpha = 0.01\n",
      "R^2: 0.680204501594487\n",
      "RMSE: 191.8862941215677\n",
      "\n",
      "\n",
      "100 iterations, alpha = 0.1\n",
      "R^2: 0.726531138813511\n",
      "RMSE: 177.44416527997652\n",
      "\n",
      "\n",
      "100 iterations, alpha = 0.5\n",
      "R^2: -2.885941549152925e+124\n",
      "RMSE: 5.764365998470874e+64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = df.shape[0]\n",
    "X_b = np.c_[np.ones((n_samples, 1)), df]\n",
    "\n",
    "\n",
    "print(\"10 iterations, alpha = 0.01\")\n",
    "weight_10_001 = my_gd(df,price_res, 0.01, 10)\n",
    "prediction_10_001 = X_b.dot(weight_10_001)\n",
    "print(\"R^2:\", r2_score(price_res, prediction_10_001))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, prediction_10_001, squared=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"10 iterations, alpha = 0.1\")\n",
    "weight_10_01 = my_gd(df,price_res, 0.1, 10)\n",
    "prediction_10_01 = X_b.dot(weight_10_01)\n",
    "print(\"R^2:\", r2_score(price_res, prediction_10_01))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, prediction_10_01, squared=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"10 iterations, alpha = 0.5\")\n",
    "weight_10_05 =my_gd(df,price_res, 0.5, 10)\n",
    "prediction_10_05 = X_b.dot(weight_10_05)\n",
    "print(\"R^2:\", r2_score(price_res, prediction_10_05))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, prediction_10_05, squared=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"50 iterations, alpha = 0.01\")\n",
    "weight_50_001 = my_gd(df,price_res, 0.01, 50)\n",
    "prediction_50_001 = X_b.dot(weight_50_001)\n",
    "print(\"R^2:\", r2_score(price_res, prediction_50_001))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, prediction_50_001, squared=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"50 iterations, alpha = 0.1\")\n",
    "weight_50_01 = my_gd(df,price_res, 0.1, 50)\n",
    "prediction_50_01 = X_b.dot(weight_50_01)\n",
    "print(\"R^2:\", r2_score(price_res, prediction_50_01))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, prediction_50_01, squared=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"50 iterations, alpha = 0.5\")\n",
    "weight_50_05 = my_gd(df,price_res, 0.5, 100)\n",
    "prediction_50_05 = X_b.dot(weight_50_05)\n",
    "print(\"R^2:\", r2_score(price_res, prediction_50_05))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, prediction_50_05, squared=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"100 iterations, alpha = 0.01\")\n",
    "weight_100_001 = my_gd(df,price_res, 0.01, 100)\n",
    "prediction_100_001 = X_b.dot(weight_100_001)\n",
    "print(\"R^2:\", r2_score(price_res, prediction_100_001))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, prediction_100_001, squared=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"100 iterations, alpha = 0.1\")\n",
    "weight_100_01 = my_gd(df,price_res, 0.1, 100)\n",
    "prediction_100_01 = X_b.dot(weight_100_01)\n",
    "print(\"R^2:\", r2_score(price_res, prediction_100_01))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, prediction_100_01, squared=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"100 iterations, alpha = 0.5\")\n",
    "weight_100_05 = my_gd(df,price_res, 0.5, 100)\n",
    "prediction_100_05 = X_b.dot(weight_100_05)\n",
    "print(\"R^2:\", r2_score(price_res, prediction_100_05))\n",
    "print(\"RMSE:\", mean_squared_error(price_res, prediction_100_05, squared=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660bf8ae",
   "metadata": {},
   "source": [
    "# Problem 5 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eab88baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a low learning rate, the algorithm shows improvement in both R^2 and RMSE as the number of iterations increases.\n",
      "This indicates that a low learning rate requires more iterations to converge but is moving towards a better solution over time.\n",
      "\n",
      "\n",
      "With a medium learning rate, the algorithm quickly converges to a more optimal solution compared to the low learning rate.\n",
      "By 50 iterations, it achieves an R^2 close to its peak at 0.726 and maintains a similar RMSE by 100 iterations. This suggests that a medium learning rate is more efficient, reaching a near-optimal solution faster.\n",
      "\n",
      "\n",
      "With a large learning rate, R^2 becomes negative and RMSE increases a lot.\n",
      "This indicates that the learning rate is too high, causing the algorithm to overshoot the minimum of the loss function, leading to divergence instead of convergence.\n",
      "\n",
      "\n",
      "When only 10 iterations are allowed, the performance depends a lot on the learning rate. Additionally, 10 iterations do not provide enough time for the algorithm to converge.\n",
      "\n",
      "\n",
      "When 50 iterations are performed, the model has more opportunity to update the weights. The models with learning rates 0.1 and 0.01 perform better and the model with alpha = 0.1 converges\n",
      "\n",
      "\n",
      "When 100 iterations are performed, diminishing returns in performance improvement, especially for the medium learning rate (0.1).\n",
      "For the low learning rate (0.01), the improvement is still positive, indicating that more iterations can be beneficial up to a point, especially for lower learning rates.\n",
      "\n",
      "\n",
      "The algorithm appears to converge to an optimal solution with a learning rate of 0.1, achieving the highest R^2 values and the lowest RMSE values at 50 and 100 iterations. This suggests that a balanced learning rate combined with a sufficient number of iterations is critical for the algorithm's success.\n"
     ]
    }
   ],
   "source": [
    "print(\"With a low learning rate, the algorithm shows improvement in both R^2 and RMSE as the number of iterations increases.\")\n",
    "print(\"This indicates that a low learning rate requires more iterations to converge but is moving towards a better solution over time.\")\n",
    "print(\"\\n\")\n",
    "print(\"With a medium learning rate, the algorithm quickly converges to a more optimal solution compared to the low learning rate.\")\n",
    "print(\"By 50 iterations, it achieves an R^2 close to its peak at 0.726 and maintains a similar RMSE by 100 iterations. This suggests that a medium learning rate is more efficient, reaching a near-optimal solution faster.\")\n",
    "print(\"\\n\")\n",
    "print(\"With a large learning rate, R^2 becomes negative and RMSE increases a lot.\")\n",
    "print(\"This indicates that the learning rate is too high, causing the algorithm to overshoot the minimum of the loss function, leading to divergence instead of convergence.\")\n",
    "print(\"\\n\")\n",
    "print(\"When only 10 iterations are allowed, the performance depends a lot on the learning rate. Additionally, 10 iterations do not provide enough time for the algorithm to converge.\")\n",
    "print(\"\\n\")\n",
    "print(\"When 50 iterations are performed, the model has more opportunity to update the weights. The models with learning rates 0.1 and 0.01 perform better and the model with alpha = 0.1 converges\")\n",
    "print(\"\\n\")\n",
    "print(\"When 100 iterations are performed, diminishing returns in performance improvement, especially for the medium learning rate (0.1).\")\n",
    "print(\"For the low learning rate (0.01), the improvement is still positive, indicating that more iterations can be beneficial up to a point, especially for lower learning rates.\")\n",
    "print(\"\\n\")\n",
    "print(\"The algorithm appears to converge to an optimal solution with a learning rate of 0.1, achieving the highest R^2 values and the lowest RMSE values at 50 and 100 iterations. This suggests that a balanced learning rate combined with a sufficient number of iterations is critical for the algorithm's success.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63152b",
   "metadata": {},
   "source": [
    "# Problem 6 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75281b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing ridge regression with gradient descent\n",
    "def my_ridge_reg(X, y, learning_rate, n_iterations, lambda_reg):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    for i in range(n_iterations):\n",
    "        predictions = X.dot(weights)\n",
    "        dW = (2/n_samples) * (X.T.dot(predictions - y)) + (lambda_reg * np.r_[0, weights[1:]])\n",
    "        weights -= learning_rate * dW\n",
    "    return weights\n",
    "\n",
    "def calculate_metrics(X, y, weights):\n",
    "    predictions = X.dot(weights)\n",
    "    r_squared = r2_score(y, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "    return r_squared, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010e47a",
   "metadata": {},
   "source": [
    "# Problem 6 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4bfa9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xi values\n",
      "[ 1.45805212  1.97824699  1.05467373 -1.56702546  1.91261659  1.8818291\n",
      "  1.42728244 -0.96370646 -1.67053837  0.7999695 ]\n",
      "Yi values\n",
      "[ 3.32455538  5.75638413  1.7237881  -3.34843221  4.41543951  6.03505308\n",
      "  1.86617133 -0.64492603 -2.96069576  3.92133704]\n"
     ]
    }
   ],
   "source": [
    "# simulating values for X_i and Y_i\n",
    "N = 1000\n",
    "low, high = -2, 2\n",
    "mu, sigma = 0, np.sqrt(2)\n",
    "Xi = np.random.uniform(low, high, N)\n",
    "ei = np.random.normal(mu, sigma, N)\n",
    "Yi = 1 + 2*Xi + ei\n",
    "print(\"Xi values\")\n",
    "print(Xi[:10])\n",
    "print(\"Yi values\")\n",
    "print(Yi[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba64d4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics after linear regression with gradient descent, self-implemented\n",
      "Intercept, Slope: [1.02045994 2.01989636]\n",
      "R^2: 0.7261850257453388\n",
      "RMSE: 1.465323936853677\n",
      "\n",
      "\n",
      "Metrics after ridge regression with gradient descent, self-implemented\n",
      "lambda = 1\n",
      "Intercept, Slope: [1.03310923 1.48714955]\n",
      "R^2: 0.6756688923392973\n",
      "RMSE: 1.5947747441224869\n",
      "\n",
      "\n",
      "lambda = 10\n",
      "Intercept, Slope: [1.058412  0.4407971]\n",
      "R^2: 0.28236397393106294\n",
      "RMSE: 2.37223201617677\n",
      "\n",
      "\n",
      "lambda = 100\n",
      "Intercept, Slope: [1.06768453 0.05485313]\n",
      "R^2: 0.0389056287018581\n",
      "RMSE: 2.745288959658938\n",
      "\n",
      "\n",
      "lambda = 1000\n",
      "Intercept, Slope: [1.06886732 0.00562274]\n",
      "R^2: 0.004037302142052779\n",
      "RMSE: 2.794644583205397\n",
      "\n",
      "\n",
      "lambda = 10000\n",
      "Intercept, Slope: [0.10172823 0.00056833]\n",
      "R^2: -0.11890143387778429\n",
      "RMSE: 2.962108496270212\n"
     ]
    }
   ],
   "source": [
    "Xi_reshaped = Xi.reshape(-1,1)\n",
    "intercept_column = np.ones((Xi_reshaped.shape[0], 1))\n",
    "X_with_intercept = np.hstack((intercept_column, Xi_reshaped))\n",
    "\n",
    "weight_50_01 = my_gd(Xi_reshaped,Yi, 0.1, 50)\n",
    "prediction_50_01 = X_with_intercept.dot(weight_50_01)\n",
    "print(\"Metrics after linear regression with gradient descent, self-implemented\")\n",
    "print(\"Intercept, Slope:\", weight_50_01)\n",
    "print(\"R^2:\", r2_score(Yi, prediction_50_01))\n",
    "print(\"RMSE:\", mean_squared_error(Yi, prediction_50_01, squared=False))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Metrics after ridge regression with gradient descent, self-implemented\")\n",
    "ridge_weights_1 = my_ridge_reg(X_with_intercept, Yi, 0.08, 50, 1)\n",
    "r_sq_1, rmse_1 = calculate_metrics(X_with_intercept, Yi, ridge_weights_1)\n",
    "print(\"lambda = 1\")\n",
    "print(\"Intercept, Slope:\", ridge_weights_1)\n",
    "print(\"R^2:\", r_sq_1)\n",
    "print(\"RMSE:\", rmse_1)\n",
    "\n",
    "ridge_weights_10 = my_ridge_reg(X_with_intercept, Yi, 0.08, 100, 10)\n",
    "r_sq_10, rmse_10 = calculate_metrics(X_with_intercept, Yi, ridge_weights_10)\n",
    "print(\"\\n\")\n",
    "print(\"lambda = 10\")\n",
    "print(\"Intercept, Slope:\", ridge_weights_10)\n",
    "print(\"R^2:\", r_sq_10)\n",
    "print(\"RMSE:\", rmse_10)\n",
    "\n",
    "ridge_weights_100 = my_ridge_reg(X_with_intercept, Yi, 0.008, 1000, 100)\n",
    "r_sq_100, rmse_100 = calculate_metrics(X_with_intercept, Yi, ridge_weights_100)\n",
    "print(\"\\n\")\n",
    "print(\"lambda = 100\")\n",
    "print(\"Intercept, Slope:\", ridge_weights_100)\n",
    "print(\"R^2:\", r_sq_100)\n",
    "print(\"RMSE:\", rmse_100)\n",
    "\n",
    "ridge_weights_1000 = my_ridge_reg(X_with_intercept, Yi, 0.0008, 10000, 1000)\n",
    "r_sq_1000, rmse_1000 = calculate_metrics(X_with_intercept, Yi, ridge_weights_1000)\n",
    "print(\"\\n\")\n",
    "print(\"lambda = 1000\")\n",
    "print(\"Intercept, Slope:\", ridge_weights_1000)\n",
    "print(\"R^2:\", r_sq_1000)\n",
    "print(\"RMSE:\", rmse_1000)\n",
    "\n",
    "ridge_weights_10000 = my_ridge_reg(X_with_intercept, Yi, 0.000005, 10000, 10000)\n",
    "r_sq_10000, rmse_10000 = calculate_metrics(X_with_intercept, Yi, ridge_weights_10000)\n",
    "print(\"\\n\")\n",
    "print(\"lambda = 10000\")\n",
    "print(\"Intercept, Slope:\", ridge_weights_10000)\n",
    "print(\"R^2:\", r_sq_10000)\n",
    "print(\"RMSE:\", rmse_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eca81dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations:\n",
      "As λ increases, the magnitude of the coefficients (slope values) decreases. This is a direct consequence of the regularization term penalizing large weights. The goal of ridge regression is to reduce model complexity and prevent overfitting by shrinking the coefficients.\n",
      "The R^2 metric decreases as λ increases, reflecting a trade-off between bias and variance. A smaller λ (closer to 0) puts more emphasis on fitting the training data (lower bias, higher variance), while a larger λ increases bias but reduces variance.\n",
      "High λ values produce a model that is too simple, significantly reducing the coefficients' values, which can result in underfitting and poor performance on both training and unseen data.\n",
      "Low λ values lead to models that are close to linear regression, focusing on reducing training error but potentially overfitting the data.\n"
     ]
    }
   ],
   "source": [
    "print(\"Observations:\")\n",
    "print(\"As λ increases, the magnitude of the coefficients (slope values) decreases. This is a direct consequence of the regularization term penalizing large weights. The goal of ridge regression is to reduce model complexity and prevent overfitting by shrinking the coefficients.\")\n",
    "print(\"The R^2 metric decreases as λ increases, reflecting a trade-off between bias and variance. A smaller λ (closer to 0) puts more emphasis on fitting the training data (lower bias, higher variance), while a larger λ increases bias but reduces variance.\")\n",
    "print(\"High λ values produce a model that is too simple, significantly reducing the coefficients' values, which can result in underfitting and poor performance on both training and unseen data.\")\n",
    "print(\"Low λ values lead to models that are close to linear regression, focusing on reducing training error but potentially overfitting the data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
